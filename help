diff --git a/branchynet/augment.py b/branchynet/augment.py
index fdb2154..33c153b 100644
--- a/branchynet/augment.py
+++ b/branchynet/augment.py
@@ -3,7 +3,7 @@ import argparse
 import six
 import numpy as np
 from skimage.io import imsave
-from scipy.misc import imresize
+from PIL import Image
 import time
 from multiprocessing import Process
 from multiprocessing import Queue, Pool
@@ -32,8 +32,10 @@ class Transform(object):
             for offset_x in six.moves.range(0, 4 + 2, 2):
                 im = img[offset_y:offset_y + self.scaling_size,
                          offset_x:offset_x + self.scaling_size]
-                im = imresize(im, (self.cropping_size, self.cropping_size),
-                              'nearest')
+                im = Image.fromarray(im)
+                im = im.resize((self.cropping_size, self.cropping_size), Image.NEAREST)
+                im = np.array(im)
+                
                 # global contrast normalization
                 im = im.astype(np.float)
                 im -= im.reshape(-1, 3).mean(axis=0)
diff --git a/branchynet/links/links.py b/branchynet/links/links.py
index a6fedc6..9e7104f 100644
--- a/branchynet/links/links.py
+++ b/branchynet/links/links.py
@@ -50,7 +50,7 @@ class X(chainer.ChainList):
     def __call__(self, x, test):
         h = x
         for link in self:
-            if len(inspect.getargspec(link.__call__)[0]) == 2:
+            if len(inspect.signature(link.__call__).parameters) == 2:
                 h = link(h)
             else:
                 h = link(h,test)
@@ -59,7 +59,7 @@ class X(chainer.ChainList):
             n, c, hh, ww = x.data.shape
             pad_c = h.data.shape[1] - c
             p = xp.zeros((n, pad_c, hh, ww), dtype=xp.float32)
-            p = chainer.Variable(p, volatile=test)
+            p = chainer.Variable(p)
             x = F.concat((p, x))
             if x.data.shape[2:] != h.data.shape[2:]:
                 x = F.average_pooling_2d(x, 1, 2)
@@ -82,16 +82,17 @@ class SL(Link):
         fnTest = copy.deepcopy(self.fnTest,memo)
         new = type(self)(fnTrain,fnTest)
         return new
-    def to_gpu(self):
-        if self.fnTrain is not None:
-            self.fnTrain.to_gpu()
-        if self.fnTest is not None:
-            self.fnTest.to_gpu()
-    def to_cpu(self):
-        if self.fnTrain is not None:
-            self.fnTrain.to_cpu()
-        if self.fnTest is not None:
-            self.fnTest.to_cpu()
+    # def to_gpu(self):
+    #     print("Here????")
+    #     if self.fnTrain is not None:
+    #         self.fnTrain.to_gpu()
+    #     if self.fnTest is not None:
+    #         self.fnTest.to_gpu()
+    # def to_cpu(self):
+    #     if self.fnTrain is not None:
+    #         self.fnTrain.to_cpu()
+    #     if self.fnTest is not None:
+    #         self.fnTest.to_cpu()
     def __call__(self, x, test=False):
         if not test:
             return self.fnTrain(x,test)
@@ -120,7 +121,7 @@ class Net(ChainList):
     def __call__(self, x, test=False, starti=0, endi=None):
         h = x
         for link in self[starti:endi]:
-            if len(inspect.getargspec(link.__call__)[0]) == 2:
+            if len(inspect.signature(link.__call__).parameters) == 2:
                 h = link(h)
             else:
                 h = link(h,test)
@@ -161,9 +162,9 @@ class Branch(ChainList):
     def __call__(self, x, test=False):
         h = x
         for link in self[starti:endi]:
-            if len(inspect.getargspec(link.__call__)[0]) == 2:
+            if len(inspect.signature(link.__call__).parameters) == 2:
                 h = link(h)
             else:
                 h = link(h,test)
         return h
-    
\ No newline at end of file
+    
diff --git a/branchynet/links/resnet.py b/branchynet/links/resnet.py
index c59f54f..64f484a 100644
--- a/branchynet/links/resnet.py
+++ b/branchynet/links/resnet.py
@@ -53,16 +53,17 @@ class ResBlock(chainer.Chain):
     def __deepcopy__(self, memo):
         new = type(self)(self.in_size, self.out_size, self.stride, self.ksize)
         return new
-    def __call__(self, x, test):
+    def __call__(self, x, test=False):
         train = not test
-        h = F.relu(self.bn1(self.conv1(x), test=not train))
-        h = self.bn2(self.conv2(h), test=not train)
+        with chainer.using_config('train', train):
+            h = F.relu(self.bn1(self.conv1(x)))
+            h = self.bn2(self.conv2(h))
         if x.data.shape != h.data.shape:
             xp = chainer.cuda.get_array_module(x.data)
             n, c, hh, ww = x.data.shape
             pad_c = h.data.shape[1] - c
             p = xp.zeros((n, pad_c, hh, ww), dtype=xp.float32)
-            p = chainer.Variable(p, volatile=not train)
+            p = chainer.Variable(p)
             x = F.concat((p, x))
             if x.data.shape[2:] != h.data.shape[2:]:
                 x = F.average_pooling_2d(x, 1, 2)
diff --git a/branchynet/net.py b/branchynet/net.py
index 5fda1bf..98872f7 100644
--- a/branchynet/net.py
+++ b/branchynet/net.py
@@ -8,8 +8,8 @@ import chainer.links as L
 from scipy.stats import entropy
 import types
 
-from links.links import *
-from functions import *
+from .links.links import *
+from .functions import *
 
 import time
 
@@ -193,8 +193,8 @@ class BranchyNet:
             if numkeep > 0:
                 xdata_keep = xdata[~idx]
                 tdata_keep = tdata[~idx]
-                remainingXVar = Variable(self.xp.array(xdata_keep,dtype=x.data.dtype),volatile=x.volatile)
-                remainingTVar = Variable(self.xp.array(tdata_keep,dtype=t.data.dtype),volatile=t.volatile)
+                remainingXVar = Variable(self.xp.array(xdata_keep,dtype=x.data.dtype))
+                remainingTVar = Variable(self.xp.array(tdata_keep,dtype=t.data.dtype))
             else:
                 remainingXVar = None
                 remainingTVar = None
@@ -202,8 +202,8 @@ class BranchyNet:
             if numexit > 0:
                 xdata_exit = xdata[idx]
                 tdata_exit = tdata[idx]                
-                exitXVar = Variable(self.xp.array(xdata_exit,dtype=x.data.dtype),volatile=x.volatile)
-                exitTVar = Variable(self.xp.array(tdata_exit,dtype=t.data.dtype),volatile=t.volatile)
+                exitXVar = Variable(self.xp.array(xdata_exit,dtype=x.data.dtype))
+                exitTVar = Variable(self.xp.array(tdata_exit,dtype=t.data.dtype))
                 
                 # if self.gpu:
                 #     exitH = model.test(exitXVar,None)
@@ -226,9 +226,9 @@ class BranchyNet:
         overall /= np.sum(numexits)
         
         if self.verbose:
-            print "numexits", numexits
-            print "accuracies", accuracies
-            print "overall accuracy", overall
+            print("numexits", numexits)
+            print("accuracies", accuracies)
+            print("overall accuracy", overall)
         
         return overall, accuracies, numexits, totaltime
     
@@ -262,7 +262,7 @@ class BranchyNet:
         
         totaltime = 0
         for i,model in enumerate(self.models):
-            if isinstance(remainingXVar,types.NoneType) or isinstance(remainingTVar,types.NoneType):
+            if isinstance(remainingXVar,type(None)) or isinstance(remainingTVar,type(None)):
                 break
             
             start_time = time.time()
@@ -314,8 +314,8 @@ class BranchyNet:
             if numkeep > 0:
                 xdata_keep = xdata[~idx]
                 tdata_keep = tdata[~idx]
-                remainingXVar = Variable(self.xp.array(xdata_keep,dtype=x.data.dtype),volatile=x.volatile)
-                remainingTVar = Variable(self.xp.array(tdata_keep,dtype=t.data.dtype),volatile=t.volatile)
+                remainingXVar = Variable(self.xp.array(xdata_keep,dtype=x.data.dtype))
+                remainingTVar = Variable(self.xp.array(tdata_keep,dtype=t.data.dtype))
             else:
                 remainingXVar = None
                 remainingTVar = None
@@ -323,8 +323,8 @@ class BranchyNet:
             if numexit > 0:
                 xdata_exit = xdata[idx]
                 tdata_exit = tdata[idx]                
-                exitXVar = Variable(self.xp.array(xdata_exit,dtype=x.data.dtype),volatile=x.volatile)
-                exitTVar = Variable(self.xp.array(tdata_exit,dtype=t.data.dtype),volatile=t.volatile)
+                exitXVar = Variable(self.xp.array(xdata_exit,dtype=x.data.dtype))
+                exitTVar = Variable(self.xp.array(tdata_exit,dtype=t.data.dtype))
                 
                 exitH = model.test(exitXVar,model.endi)
                 hs.append(exitH.data)
@@ -347,7 +347,7 @@ class BranchyNet:
             accuracydata = accuracy.data
         
         if self.verbose:
-            print "accuracies", accuracydata
+            print("accuracies", accuracydata)
             
         return accuracydata, totaltime
     
@@ -365,8 +365,8 @@ class BranchyNet:
             accuraciesdata = accuracy.data            
 
         if self.verbose:        
-            print "losses",lossesdata
-            print "accuracies",accuraciesdata
+            print("losses",lossesdata)
+            print("accuracies",accuraciesdata)
         
         return lossesdata,accuraciesdata
         
@@ -427,7 +427,7 @@ class BranchyNet:
 
             numsamples = x.data.shape[0]
             for i,model in enumerate(self.models[:-1]):
-                if isinstance(remainingXVar,types.NoneType) or isinstance(remainingTVar,types.NoneType):
+                if isinstance(remainingXVar,type(None)) or isinstance(remainingTVar,type(None)):
                     break
                 loss = model.train(remainingXVar,remainingTVar)
 
@@ -462,7 +462,6 @@ class BranchyNet:
         return entropies
             
     def train(self,x,t=None):
-        
         # SCATTER: copy params
         for i,link in enumerate(self.main):
             for model in self.models:
@@ -489,7 +488,7 @@ class BranchyNet:
         nummodels = len(self.models)
         numsamples = x.data.shape[0]
         for i,model in enumerate(self.models):
-            if isinstance(remainingXVar,types.NoneType) or isinstance(remainingTVar,types.NoneType):
+            if isinstance(remainingXVar,type(None)) or isinstance(remainingTVar,type(None)):
                 break
             loss = model.train(remainingXVar,remainingTVar)
             losses.append(loss)
@@ -546,8 +545,8 @@ class BranchyNet:
                 tdata = remainingTVar.data
             
             if numkeep > 0:
-                remainingXVar = Variable(self.xp.array(xdata[~idx]),volatile=x.volatile)
-                remainingTVar = Variable(self.xp.array(tdata[~idx]),volatile=t.volatile)
+                remainingXVar = Variable(self.xp.array(xdata[~idx]))
+                remainingTVar = Variable(self.xp.array(tdata[~idx]))
             else:
                 remainingXVar = None
                 remainingTVar = None
@@ -605,19 +604,19 @@ class BranchyNet:
             accuraciesdata = [accuracy.data for accuracy in accuracies]
         
         if self.verbose:
-            print "numexits",numexits
-            print "losses",lossesdata
-            print "accuracies",accuraciesdata
+            print("numexits",numexits)
+            print("losses",lossesdata)
+            print("accuracies",accuraciesdata)
             
         return lossesdata,accuraciesdata
     
     def print_models(self):
         for model in self.models:
-            print "----", model.starti, model.endi
+            print("----", model.starti, model.endi)
             for link in model:
-                print link
-        print "----", self.main.starti, model.endi
+                print(link)
+        print("----", self.main.starti, model.endi)
         for link in self.main:
-            print link
-        print "----"
+            print(link)
+        print("----")
         
\ No newline at end of file
diff --git a/branchynet/utils.py b/branchynet/utils.py
index e5db8fb..02fc3a9 100644
--- a/branchynet/utils.py
+++ b/branchynet/utils.py
@@ -3,7 +3,7 @@ import numpy as np
 import time
 from itertools import product
 import chainer.functions as F
-from augment import augmentation
+from .augment import augmentation
 
 def test_suite_A(branchyNet,x_test,y_test,batchsize=10000,ps=np.linspace(0.1,1.0,10)):
     accs = []
@@ -47,8 +47,8 @@ def test_augment(branchyNet,x_test,y_test=None,batchsize=10000,main=False):
         x=branchyNet.xp.asarray(x,dtype=branchyNet.xp.float32)
         t=branchyNet.xp.asarray(t,dtype=branchyNet.xp.int32)
                 
-        x = Variable(x, volatile=True)
-        t = Variable(t, volatile=True)
+        x = Variable(x)
+        t = Variable(t)
 
         # start_time = time.time()
         if main:
@@ -58,7 +58,7 @@ def test_augment(branchyNet,x_test,y_test=None,batchsize=10000,main=False):
         
         totaltime += branchyNet.runtime
         
-        print "pred.shape",pred.shape
+        print("pred.shape",pred.shape)
         pred = pred.mean(axis=0)
         acc = int(pred.argmax() == t.data[0])
         sum_accuracy += acc
@@ -88,8 +88,8 @@ def test(branchyNet,x_test,y_test=None,batchsize=10000,main=False):
         input_data = branchyNet.xp.asarray(input_data, dtype=branchyNet.xp.float32)
         label_data = branchyNet.xp.asarray(label_data, dtype=branchyNet.xp.int32)
 
-        x = Variable(input_data, volatile=True)
-        t = Variable(label_data, volatile=True)
+        x = Variable(input_data)
+        t = Variable(label_data)
         if main:
             acc, diff = branchyNet.test_main(x,t)
             #if hasattr(h.data,'get'):
@@ -117,8 +117,8 @@ def test(branchyNet,x_test,y_test=None,batchsize=10000,main=False):
         if num_exits[i] > 0:
             accbreakdowns[i]/=num_exits[i]
     #if len(finals) > 0:
-    #    hh = Variable(np.vstack(finals),volatile=True)
-    #    tt = Variable(y_test, volatile=True)
+    #    hh = Variable(np.vstack(finals))
+    #    tt = Variable(y_test)
     #    overall = F.accuracy(hh,tt).data
     
     return overall, totaltime, num_exits, accbreakdowns
@@ -130,7 +130,7 @@ def get_SM(branchyNet, x_test, batchsize=10000):
     for i in range(0, datasize, batchsize):
         input_data = x_test[i : i + batchsize]
         input_data = branchyNet.xp.asarray(input_data, dtype=branchyNet.xp.float32)
-        x = Variable(input_data, volatile=True)
+        x = Variable(input_data)
         exitHs.extend(branchyNet.get_SM(x))
 
     return exitHs
@@ -140,11 +140,11 @@ def traintest_augment(branchyNet,x_train,y_train,x_test,y_test,batchsize=10000,n
     for i in range(num_epoch):
         branchyNet.training()
         plotlosses,plotaccuracies,totaltime = train_augment(branchyNet,x_train,y_train,batchsize=batchsize,num_epoch=1,main=main)
-        print("train losses", plotlosses)
-        print("train accuracy", plotaccuracies)
+        print(("train losses", plotlosses))
+        print(("train accuracy", plotaccuracies))
         branchyNet.testing()
         overall, totaltime, num_exits = test_augment(branchyNet,x_test,y_test,batchsize=batchsize,main=main)
-        print("test accuracy", overall)
+        print(("test accuracy", overall))
 
     return plotlosses,plotaccuracies,totaltime
 
@@ -159,8 +159,8 @@ def traintest(branchyNet,x_train,y_train,x_test,y_test,batchsize=10000,num_epoch
         branchyNet.training()
         tr_loss, tr_acc, rt = train(branchyNet,x_train,y_train,batchsize=batchsize,num_epoch=1,main=main)
         if verbose:
-            print("train losses", tr_loss[0])
-            print("train accuracy", tr_acc[0])
+            print(("train losses", tr_loss[0]))
+            print(("train accuracy", tr_acc[0]))
         total_time += rt
         losses.append(tr_loss[0])
         accs.append(tr_acc[0])
@@ -168,9 +168,9 @@ def traintest(branchyNet,x_train,y_train,x_test,y_test,batchsize=10000,num_epoch
         branchyNet.testing()
         t_acc, t_time, t_exits, t_accbreakdowns = test(branchyNet,x_test,y_test,batchsize=batchsize,main=main)
         if verbose:
-            print("test accuracy", t_accbreakdowns)
-            print("test exits", t_exits)
-            print("test accuracy overall", t_acc)
+            print(("test accuracy", t_accbreakdowns))
+            print(("test exits", t_exits))
+            print(("test accuracy overall", t_acc))
         test_totaltime += t_time
         test_overall.append(t_acc)
         test_num_exits.append(t_exits)
@@ -181,7 +181,7 @@ def traintest(branchyNet,x_train,y_train,x_test,y_test,batchsize=10000,num_epoch
 def train_augment(branchyNet,x_train,y_train,batchsize=10000,num_epoch=20,main=False):
     datasize = x_train.shape[0]
     
-    plotepochs = range(num_epoch)
+    plotepochs = list(range(num_epoch))
     plotlosses = []
     plotaccuracies = []
     # plotnumsamples = []
@@ -213,7 +213,7 @@ def train_augment(branchyNet,x_train,y_train,batchsize=10000,num_epoch=20,main=F
 def train(branchyNet,x_train,y_train,batchsize=10000,num_epoch=20,main=False):
     datasize = x_train.shape[0]
     
-    plotepochs = range(num_epoch)
+    plotepochs = list(range(num_epoch))
     plotlosses = []
     plotaccuracies = []
     # plotnumsamples = []
@@ -230,7 +230,6 @@ def train(branchyNet,x_train,y_train,batchsize=10000,num_epoch=20,main=False):
         avgaccuracies = []
         # avgnumsamples = []
         # avgexitsamples = []
-
         for i in range(0, datasize, batchsize):
             input_data = x_train[indexes[i : i + batchsize]]
             label_data = y_train[indexes[i : i + batchsize]]
@@ -309,13 +308,13 @@ def screen_branchy(branchyNet, x_test, y_test, base_ts, batchsize=1, enumerate_t
 def branchy_table_results(network, baseacc, basediff, accs, diffs, exits, ts):
     print_lst = lambda xs: '{' + ', '.join(map(str, xs)) + '}'
 
-    print '{:>15}{:>15}{:>15}{:>15}{:>15}{:>15}'.format('Network', 'Acc.(%)', 'Time(ms)', 'Gain', 'Thrshld.T', 'Exit(%)')
-    print '{:>15}{:>15.2f}{:>15.2f}{:>15}{:>15}{:>15}'.format(network, baseacc*100., basediff, '-', '-', '-')
+    print('{:>15}{:>15}{:>15}{:>15}{:>15}{:>15}'.format('Network', 'Acc.(%)', 'Time(ms)', 'Gain', 'Thrshld.T', 'Exit(%)'))
+    print('{:>15}{:>15.2f}{:>15.2f}{:>15}{:>15}{:>15}'.format(network, baseacc*100., basediff, '-', '-', '-'))
     
     for i, (acc, diff, exit, t) in enumerate(zip(accs, diffs, exits, ts)):
-        print '{:>15}{:>15.2f}{:>15.2f}{:>15.2f}{:>15}{:>15}'.format('B-'+network, acc*100., diff, basediff / diff, 
+        print('{:>15}{:>15.2f}{:>15.2f}{:>15.2f}{:>15}{:>15}'.format('B-'+network, acc*100., diff, basediff / diff, 
                                                                      print_lst(t), 
-                                                                     print_lst(100.*(exit/float(sum(exit)))))
+                                                                     print_lst(100.*(exit/float(sum(exit))))))
             
 def compute_network_times(exits, branch_times):
     total_times = []
@@ -334,7 +333,7 @@ def compute_network_times(exits, branch_times):
 def compute_branch_times(net, x_test, y_test, batchsize=1, num_samples=200):
     thresholds = [10.]
     branch_times = []
-    for i in xrange(len(net.models)):
+    for i in range(len(net.models)):
         net.thresholdExits = thresholds
         _, branch_time, _ = test(net, x_test[:num_samples], y_test[:num_samples],
                                  batchsize=batchsize, main=False)
diff --git a/branchynet/visualize.py b/branchynet/visualize.py
index 7482f48..7e2465f 100644
--- a/branchynet/visualize.py
+++ b/branchynet/visualize.py
@@ -1,7 +1,7 @@
 import matplotlib.pyplot as plt
 import matplotlib
 import numpy as np
-import utils
+from . import utils
 
 from chainer import Variable
 import chainer.functions as F
@@ -93,14 +93,7 @@ def plot_roc(ps,accs,diffs,baseacc,basediff):
 def plot_line_tradeoff(accs, diffs, ps, exits, baseacc, basediff, orig_label='Baseline', title=None, our_label='Our Method',
                        xlabel='Runtime (s)', ylabel='Classification Accuracy', all_samples=False, knee_idx=None,
                        xlim=None, ylim=None, inc_amt=-0.0005, output_path=None):
-    matplotlib.rcParams.update({'axes.labelsize': 10,
-    'text.fontsize': 18,
-    'legend.fontsize': 15,
-    'xtick.labelsize': 13,
-    'ytick.labelsize': 13,
-    'axes.labelsize': 18,
-    'text.usetex': False,
-    'figure.figsize': [4.5, 3.5]})
+    matplotlib.rcParams.update({'axes.labelsize': 10})
 
     matplotlib.rcParams['legend.numpoints'] = 1
     matplotlib.rcParams['pdf.fonttype'] = 42
@@ -149,7 +142,7 @@ def plot_layer_entropy(leakyNet, x):
     
     leakyNet.to_cpu()
     x = leakyNet.xp.asarray(x, dtype=leakyNet.xp.float32)
-    h = Variable(x, volatile=True)
+    h = Variable(x)
     ents = []
     for model in leakyNet.models:
         h = model.test(h,model.starti,model.endi)
diff --git a/datasets/mnist.py b/datasets/mnist.py
index 58ac3c0..727769a 100644
--- a/datasets/mnist.py
+++ b/datasets/mnist.py
@@ -1,8 +1,10 @@
-from sklearn.datasets import fetch_mldata
+from sklearn.datasets import fetch_openml
 import numpy as np
+import ssl
 
 def get_data():
-    mnist = fetch_mldata('MNIST original')
+    ssl._create_default_https_context = ssl._create_unverified_context
+    mnist = fetch_openml(name='mnist_784', version=1, as_frame=False)
     x_all = mnist['data'].astype(np.float32) / 255
     y_all = mnist['target'].astype(np.int32)
     x_train, x_test = np.split(x_all, [60000])
diff --git a/datasets/pcifar10.py b/datasets/pcifar10.py
index 7344c2d..7696a82 100644
--- a/datasets/pcifar10.py
+++ b/datasets/pcifar10.py
@@ -5,16 +5,34 @@ import numpy as np
 from six.moves import cPickle as pickle
 from scipy import linalg
 
+from chainer.datasets import get_cifar10
+from chainer.dataset import concat_examples
+from chainer.iterators import SerialIterator
+
 dirname = os.path.dirname(os.path.realpath(__file__))
 
-def get_data(redir=''):
-    datasets = np.load(redir + 'datasets/data/pcifar10/data.npz')
-    train_data = datasets['train_x']
-    train_labels = datasets['train_y']
-    test_data = datasets['test_x']
-    test_labels = datasets['test_y']
+def get_data():
+    # Automatically download and split CIFAR-10 dataset
+    train, test = get_cifar10()
+    
+    # Extract data and labels
+    train_data, train_labels = concat_examples(train)
+    test_data, test_labels = concat_examples(test)
+
+    # Normalize data to [0, 1] range
+    train_data = train_data.astype(np.float32) / 255.0
+    test_data = test_data.astype(np.float32) / 255.0
+
     return train_data, train_labels, test_data, test_labels
 
+# def get_data(redir=''):
+#     datasets = np.load(redir + 'datasets/data/pcifar10/data.npz')
+#     train_data = datasets['train_x']
+#     train_labels = datasets['train_y']
+#     test_data = datasets['test_x']
+#     test_labels = datasets['test_y']
+#     return train_data, train_labels, test_data, test_labels
+
 def get_data_dev(numclasses=2):
     train_data, train_labels, test_data, test_labels = get_data()
     idx = (train_labels == 0)
diff --git a/experiment_alex_cifar10.py b/experiment_alex_cifar10.py
index a6ba0aa..b59d71d 100644
--- a/experiment_alex_cifar10.py
+++ b/experiment_alex_cifar10.py
@@ -12,9 +12,14 @@ from chainer import cuda
 
 from networks import alex_cifar10
 
+print("Get network")
 branchyNet = alex_cifar10.get_network()
 if cuda.available:
+    print("to gpu")
     branchyNet.to_gpu()
+
+print("BranchyNet Training")
+
 branchyNet.training()
 
 
@@ -24,6 +29,7 @@ branchyNet.training()
 
 from datasets import pcifar10
 
+print("Import data")
 x_train,y_train,x_test,y_test = pcifar10.get_data()
 
 
@@ -39,19 +45,29 @@ TRAIN_NUM_EPOCHS = 50
 # Train Main Network
 
 # In[ ]:
+print("Main train")
 
 main_loss, main_acc, main_time = utils.train(branchyNet, x_train, y_train, main=True, batchsize=TRAIN_BATCHSIZE,
                                              num_epoch=TRAIN_NUM_EPOCHS)
 
+print("Main loss: ", main_loss)
+print("Main acc: ", main_acc)
+print("Main time: ", main_time)
 
 # Train BranchyNet
 
 # In[ ]:
+print("Branch train")
 
 TRAIN_NUM_EPOCHS = 100
 branch_loss, branch_acc, branch_time = utils.train(branchyNet, x_train, y_train, batchsize=TRAIN_BATCHSIZE,
                                              num_epoch=TRAIN_NUM_EPOCHS)
 
+print("Branch loss: ", branch_loss)
+print("Branch acc: ", branch_acc)
+print("Branch time: ", branch_time)
+
+print("BranchyNet.testing")
 #set network to inference mode
 branchyNet.testing()
 
@@ -59,6 +75,7 @@ branchyNet.testing()
 # Visualizing Network Training
 
 # In[ ]:
+print("Plot layers")
 
 visualize.plot_layers(main_loss, xlabel='Epochs', ylabel='Training Loss')
 visualize.plot_layers(main_acc, xlabel='Epochs', ylabel='Training Accuracy')
@@ -66,8 +83,8 @@ visualize.plot_layers(main_acc, xlabel='Epochs', ylabel='Training Accuracy')
 
 # In[ ]:
 
-visualize.plot_layers(zip(*branch_loss), xlabel='Epochs', ylabel='Training Loss')
-visualize.plot_layers(zip(*branch_acc), xlabel='Epochs', ylabel='Training Accuracy')
+visualize.plot_layers(list(zip(*branch_loss)), xlabel='Epochs', ylabel='Training Loss')
+visualize.plot_layers(list(zip(*branch_acc)), xlabel='Epochs', ylabel='Training Accuracy')
 
 
 # Run test suite and visualize
@@ -79,10 +96,14 @@ branchyNet.testing()
 branchyNet.verbose = False
 if cuda.available:
     branchyNet.to_gpu()
+print("Utils test")
+
 g_baseacc, g_basediff, _, _ = utils.test(branchyNet,x_test,y_test,main=True,batchsize=TEST_BATCHSIZE)
 g_basediff = (g_basediff / float(len(y_test))) * 1000.
 
-#branchyNet.to_cpu()
+print("Utils test 2")
+
+branchyNet.to_cpu()
 c_baseacc, c_basediff, _, _ = utils.test(branchyNet,x_test,y_test,main=True,batchsize=TEST_BATCHSIZE)
 c_basediff = (c_basediff / float(len(y_test))) * 1000.
 
@@ -96,6 +117,7 @@ thresholds = [0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 0.75, 1., 5., 10.]
 # In[ ]:
 
 #GPU
+print("Screen branchy")
 if cuda.available:
     branchyNet.to_gpu()
 g_ts, g_accs, g_diffs, g_exits = utils.screen_branchy(branchyNet, x_test, y_test, thresholds,
@@ -115,6 +137,7 @@ visualize.plot_line_tradeoff(g_accs, g_diffs, g_ts, g_exits, g_baseacc, g_basedi
 
 
 # In[ ]:
+print("Screen branchy 2")
 
 #CPU
 branchyNet.to_cpu()
@@ -143,7 +166,7 @@ utils.branchy_table_results(c_baseacc, c_basediff, g_basediff, c_accs, c_diffs,
 # Save model/data
 
 # In[ ]:
-
+print("Saving data")
 import dill
 branchyNet.to_cpu()
 with open("_models/alexnet_cifar10.bn", "w") as f:
@@ -152,4 +175,3 @@ with open("_models/alexnet_cifar10_gpu_results.pkl", "w") as f:
     dill.dump({'accs': g_accs, 'rt': g_diffs, 'exits': g_exits, 'ts': g_ts, 'baseacc': g_baseacc, 'basediff': g_basediff}, f)
 with open("_models/alexnet_cifar10_cpu_results.pkl", "w") as f:
     dill.dump({'accs': c_accs, 'rt': c_diffs, 'exits': c_exits, 'ts': c_ts, 'baseacc': c_baseacc, 'basediff': c_basediff}, f)
-
diff --git a/experiment_lenet_mnist.py b/experiment_lenet_mnist.py
index f86afc9..2479372 100644
--- a/experiment_lenet_mnist.py
+++ b/experiment_lenet_mnist.py
@@ -12,9 +12,13 @@ from chainer import cuda
 
 from networks import lenet_mnist
 
+print("Get network")
 branchyNet = lenet_mnist.get_network()
 if cuda.available:
+    print("to gpu")
     branchyNet.to_gpu()
+print("Training")
+
 branchyNet.training()
 
 
@@ -22,6 +26,8 @@ branchyNet.training()
 
 # In[4]:
 
+print("Get data")
+
 from datasets import mnist
 x_train, y_train, x_test, y_test = mnist.get_data()
 
@@ -38,26 +44,41 @@ TRAIN_NUM_EPOCHS = 50
 # Train Main Network
 
 # In[6]:
-
+print("Training main network")
 main_loss, main_acc, main_time = utils.train(branchyNet, x_train, y_train, main=True, batchsize=TRAIN_BATCHSIZE,
                                              num_epoch=TRAIN_NUM_EPOCHS)
 
-
+print("Main loss: ", main_loss)
+print("Main acc: ", main_acc)
+print("Main time: ", main_time)
 # Train BranchyNet
 
 # In[7]:
+print("Training branchynet network")
 
 TRAIN_NUM_EPOCHS = 100
 branch_loss, branch_acc, branch_time = utils.train(branchyNet, x_train, y_train, batchsize=TRAIN_BATCHSIZE,
                                              num_epoch=TRAIN_NUM_EPOCHS)
+print("Branch loss: ", branch_loss)
+print("Branch acc: ", branch_acc)
+print("Branch time: ", branch_time)
+
+import dill
+branchyNet.to_cpu()
+with open("_models/lenet_mnist.bn", "wb") as f:
+    dill.dump(branchyNet, f)
 
 #set network to inference mode
+print("BranchyNet.testing()")
+
 branchyNet.testing()
 
 
 # Visualizing Network Training
 
+
 # In[8]:
+print("Attempt to plot")
 
 visualize.plot_layers(main_loss, xlabel='Epochs', ylabel='Training Loss')
 visualize.plot_layers(main_acc, xlabel='Epochs', ylabel='Training Accuracy')
@@ -65,34 +86,46 @@ visualize.plot_layers(main_acc, xlabel='Epochs', ylabel='Training Accuracy')
 
 # In[9]:
 
-visualize.plot_layers(zip(*branch_loss), xlabel='Epochs', ylabel='Training Loss')
-visualize.plot_layers(zip(*branch_acc), xlabel='Epochs', ylabel='Training Accuracy')
+visualize.plot_layers(list(zip(*branch_loss)), xlabel='Epochs', ylabel='Training Loss')
+visualize.plot_layers(list(zip(*branch_acc)), xlabel='Epochs', ylabel='Training Accuracy')
 
 
 # Run test suite and visualize
 
 # In[11]:
 
+print("test suite")
 #set network to inference mode
 branchyNet.testing()
 branchyNet.verbose = False
 if cuda.available:
     branchyNet.to_gpu()
+
+print("utils test")
+
 g_baseacc, g_basediff, _, _ = utils.test(branchyNet,x_test,y_test,main=True,batchsize=TEST_BATCHSIZE)
 g_basediff = (g_basediff / float(len(y_test))) * 1000.
 
-#branchyNet.to_cpu()
-c_baseacc, c_basediff, _, _ = utils.test(branchyNet,x_test,y_test,main=True,batchsize=TEST_BATCHSIZE)
-c_basediff = (c_basediff / float(len(y_test))) * 1000.
+print("g_baseacc: ", g_baseacc)
+print("g_basediff: ", g_basediff)
+
+print("utils test 2")
+
+# #branchyNet.to_cpu()
+# c_baseacc, c_basediff, _, _ = utils.test(branchyNet,x_test,y_test,main=True,batchsize=TEST_BATCHSIZE)
+# c_basediff = (c_basediff / float(len(y_test))) * 1000.
 
+# print("c base acc: ", c_baseacc)
+# print("c base diff: ", c_basediff)
 
-# In[30]:
+# # In[30]:
 
-# Specify thresholds
-thresholds = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1., 2., 3., 5., 10.]
+# # Specify thresholds
+# thresholds = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1., 2., 3., 5., 10.]
 
 
 # In[20]:
+print("screen branchy")
 
 #GPU
 if cuda.available:
@@ -102,30 +135,59 @@ g_ts, g_accs, g_diffs, g_exits = utils.screen_branchy(branchyNet, x_test, y_test
 # g_ts, g_accs, g_diffs, g_exits = utils.screen_leaky(leakyNet, x_test, y_test, thresholds, inc_amt=-0.1,
 #                                                     batchsize=TEST_BATCHSIZE, verbose=True)
 
+print("g_ts: ", g_ts)
+print("g_accs: ", g_accs)
+print("g_diffs: ", g_diffs)
+print("g_exits: ", g_exits)
 #convert to ms
 g_diffs *= 1000.
 
+print("plot shit")
 
 # In[ ]:
 
-visualize.plot_line_tradeoff(g_accs, g_diffs, g_ts, g_exits, g_baseacc, g_basediff, all_samples=False, inc_amt=-0.0001000,
-                             our_label='BranchyLeNet', orig_label='LeNet', xlabel='Runtime (ms)', 
-                             title='LeNet GPU', output_path='_figs/lenet_gpu.pdf')
+# In[32]:
+# print("screen branchy 2")
 
+# #CPU
+# branchyNet.to_cpu()
+# c_ts, c_accs, c_diffs, c_exits  = utils.screen_branchy(branchyNet, x_test, y_test, thresholds,
+#                                                      batchsize=TEST_BATCHSIZE, verbose=True)
+# # c_ts, c_accs, c_diffs, c_exits  = utils.screen_branchy(branchyNet, x_test, y_test, g_ts, inc_amt=0.01,
+# #                                                      batchsize=TEST_BATCHSIZE, prescreen=False, verbose=True)
+# #convert to ms
+# c_diffs *= 1000.
 
-# In[32]:
 
-#CPU
+# # In[22]:
+# print("plot again")
+
+# print("c_accs: ", c_accs)
+# print("c_diffs: ", c_diffs)
+# print("c_ts: ", c_ts)
+# print("c_exits: ", c_exits)
+# print("c_baseacc: ", c_baseacc)
+# print("c_basediff: ", c_basediff)
+
+# Save model/data
+
+# In[40]:
+print("dill dump")
+
+import dill
 branchyNet.to_cpu()
-c_ts, c_accs, c_diffs, c_exits  = utils.screen_branchy(branchyNet, x_test, y_test, thresholds,
-                                                     batchsize=TEST_BATCHSIZE, verbose=True)
-# c_ts, c_accs, c_diffs, c_exits  = utils.screen_branchy(branchyNet, x_test, y_test, g_ts, inc_amt=0.01,
-#                                                      batchsize=TEST_BATCHSIZE, prescreen=False, verbose=True)
-#convert to ms
-c_diffs *= 1000.
+with open("_models/lenet_mnist.bn", "wb") as f:
+    dill.dump(branchyNet, f)
+with open("_models/lenet_mnist_gpu_results.pkl", "w") as f:
+    dill.dump({'accs': g_accs, 'rt': g_diffs, 'exits': g_exits, 'ts': g_ts, 'baseacc': g_baseacc, 'basediff': g_basediff}, f)
+# with open("_models/lenet_mnist_cpu_results.pkl", "w") as f:
+#     dill.dump({'accs': c_accs, 'rt': c_diffs, 'exits': c_exits, 'ts': c_ts, 'baseacc': c_baseacc, 'basediff': c_basediff}, f)
+
+visualize.plot_line_tradeoff(g_accs, g_diffs, g_ts, g_exits, g_baseacc, g_basediff, all_samples=False, inc_amt=-0.0001000,
+                             our_label='BranchyLeNet', orig_label='LeNet', xlabel='Runtime (ms)', 
+                             title='LeNet GPU', output_path='_figs/lenet_gpu.pdf')
 
 
-# In[22]:
 
 visualize.plot_line_tradeoff(c_accs, c_diffs, c_ts, c_exits, c_baseacc, c_basediff, all_samples=False, inc_amt=-0.0001000,
                              our_label='BranchyLeNet', orig_label='LeNet', xlabel='Runtime (ms)',
@@ -134,21 +196,11 @@ visualize.plot_line_tradeoff(c_accs, c_diffs, c_ts, c_exits, c_baseacc, c_basedi
 
 # In[ ]:
 
+print("branchy table results")
+
 #Compute table results
 utils.branchy_table_results(c_baseacc, c_basediff, g_basediff, c_accs, c_diffs, g_accs, g_diffs, inc_amt=0.000, 
                           network='LeNet')
 
 
-# Save model/data
-
-# In[40]:
-
-import dill
-branchyNet.to_cpu()
-with open("_models/lenet_mnist.bn", "w") as f:
-    dill.dump(branchyNet, f)
-with open("_models/lenet_mnist_gpu_results.pkl", "w") as f:
-    dill.dump({'accs': g_accs, 'rt': g_diffs, 'exits': g_exits, 'ts': g_ts, 'baseacc': g_baseacc, 'basediff': g_basediff}, f)
-with open("_models/lenet_mnist_cpu_results.pkl", "w") as f:
-    dill.dump({'accs': c_accs, 'rt': c_diffs, 'exits': c_exits, 'ts': c_ts, 'baseacc': c_baseacc, 'basediff': c_basediff}, f)
 
diff --git a/experiment_resnet_cifar10.py b/experiment_resnet_cifar10.py
index 1ba2827..30fba5e 100644
--- a/experiment_resnet_cifar10.py
+++ b/experiment_resnet_cifar10.py
@@ -12,15 +12,21 @@ from chainer import cuda
 
 from networks import resnet_cifar10
 
+print("Get network")
+
 branchyNet = resnet_cifar10.get_network()
 if cuda.available:
+    print("to gpu")
     branchyNet.to_gpu()
+
+print("Training")
 branchyNet.training()
 
 
 # Import Data
 
 # In[4]:
+print("Get data")
 
 from datasets import pcifar10
 
@@ -39,61 +45,80 @@ TRAIN_NUM_EPOCHS = 100
 # Train Main Network
 
 # In[ ]:
+print("Training main network")
 
 main_loss, main_acc, main_time = utils.train(branchyNet, x_train, y_train, main=True, batchsize=TRAIN_BATCHSIZE,
                                              num_epoch=TRAIN_NUM_EPOCHS)
 
-
+print("Main loss: ", main_loss)
+print("Main acc: ", main_acc)
+print("Main time: ", main_time)
 # Train BranchyNet
 
 # In[ ]:
 
 TRAIN_NUM_EPOCHS = 100
+print("Training branchynet network")
+
 branch_loss, branch_acc, branch_time = utils.train(branchyNet, x_train, y_train, batchsize=TRAIN_BATCHSIZE,
                                              num_epoch=TRAIN_NUM_EPOCHS)
+print("Branch loss: ", branch_loss)
+print("Branch acc: ", branch_acc)
+print("Branch time: ", branch_time)
 
 #set network to inference mode
 branchyNet.testing()
 
+print("BranchyNet.testing()")
 
 # Visualizing Network Training
 
 # In[ ]:
-
+print("Attempt to plot")
 visualize.plot_layers(main_loss, xlabel='Epochs', ylabel='Training Loss')
 visualize.plot_layers(main_acc, xlabel='Epochs', ylabel='Training Accuracy')
 
 
 # In[ ]:
 
-visualize.plot_layers(zip(*branch_loss), xlabel='Epochs', ylabel='Training Loss')
-visualize.plot_layers(zip(*branch_acc), xlabel='Epochs', ylabel='Training Accuracy')
+visualize.plot_layers(list(zip(*branch_loss)), xlabel='Epochs', ylabel='Training Loss')
+visualize.plot_layers(list(zip(*branch_acc)), xlabel='Epochs', ylabel='Training Accuracy')
 
 
 # Run test suite and visualize
 
 # In[ ]:
 
+print("test suite")
+
 #set network to inference mode
 branchyNet.testing()
 branchyNet.verbose = False
 if cuda.available:
     branchyNet.to_gpu()
+
+print("utils test")
 g_baseacc, g_basediff, _, _ = utils.test(branchyNet,x_test,y_test,main=True,batchsize=TEST_BATCHSIZE)
 g_basediff = (g_basediff / float(len(y_test))) * 1000.
 
-#branchyNet.to_cpu()
-c_baseacc, c_basediff, _, _ = utils.test(branchyNet,x_test,y_test,main=True,batchsize=TEST_BATCHSIZE)
-c_basediff = (c_basediff / float(len(y_test))) * 1000.
+print("g_baseacc: ", g_baseacc)
+print("g_basediff: ", g_basediff)
 
+print("utils test 2")
 
-# In[ ]:
+# #branchyNet.to_cpu()
+# c_baseacc, c_basediff, _, _ = utils.test(branchyNet,x_test,y_test,main=True,batchsize=TEST_BATCHSIZE)
+# c_basediff = (c_basediff / float(len(y_test))) * 1000.
+
+
+# # In[ ]:
 
-# Specify thresholds
-thresholds = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1., 2., 3., 5., 10.]
+# # Specify thresholds
+# thresholds = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1., 2., 3., 5., 10.]
 
 
 # In[ ]:
+print("screen branchy")
 
 #GPU
 if cuda.available:
@@ -103,10 +128,32 @@ g_ts, g_accs, g_diffs, g_exits = utils.screen_branchy(branchyNet, x_test, y_test
 # g_ts, g_accs, g_diffs, g_exits = utils.screen_leaky(leakyNet, x_test, y_test, thresholds, inc_amt=-0.1,
 #                                                     batchsize=TEST_BATCHSIZE, verbose=True)
 
+print("g_ts: ", g_ts)
+print("g_accs: ", g_accs)
+print("g_diffs: ", g_diffs)
+print("g_exits: ", g_exits)
 #convert to ms
 g_diffs *= 1000.
 
 
+# Save model/data
+
+# In[ ]:
+print("Dill dump")
+import dill
+branchyNet.to_cpu()
+with open("_models/resnet_cifar10.bn", "w") as f:
+    dill.dump(branchyNet, f)
+with open("_models/resnet_cifar10_gpu_results.pkl", "w") as f:
+    dill.dump({'accs': g_accs, 'rt': g_diffs, 'exits': g_exits, 'ts': g_ts, 'baseacc': g_baseacc, 'basediff': g_basediff}, f)
+with open("_models/resnet_cifar10_cpu_results.pkl", "w") as f:
+    dill.dump({'accs': c_accs, 'rt': c_diffs, 'exits': c_exits, 'ts': c_ts, 'baseacc': c_baseacc, 'basediff': c_basediff}, f)
+
+
+
+print("plot shit")
+
+
 # In[ ]:
 
 visualize.plot_line_tradeoff(g_accs, g_diffs, g_ts, g_exits, g_baseacc, g_basediff, all_samples=False, inc_amt=-0.0001000,
@@ -140,16 +187,3 @@ utils.branchy_table_results(c_baseacc, c_basediff, g_basediff, c_accs, c_diffs,
                           network='ResNet')
 
 
-# Save model/data
-
-# In[ ]:
-
-import dill
-branchyNet.to_cpu()
-with open("_models/resnet_cifar10.bn", "w") as f:
-    dill.dump(branchyNet, f)
-with open("_models/resnet_cifar10_gpu_results.pkl", "w") as f:
-    dill.dump({'accs': g_accs, 'rt': g_diffs, 'exits': g_exits, 'ts': g_ts, 'baseacc': g_baseacc, 'basediff': g_basediff}, f)
-with open("_models/resnet_cifar10_cpu_results.pkl", "w") as f:
-    dill.dump({'accs': c_accs, 'rt': c_diffs, 'exits': c_exits, 'ts': c_ts, 'baseacc': c_baseacc, 'basediff': c_basediff}, f)
-
diff --git a/networks/alex_cifar10.py b/networks/alex_cifar10.py
index 6a0890c..207ccc5 100644
--- a/networks/alex_cifar10.py
+++ b/networks/alex_cifar10.py
@@ -1,4 +1,4 @@
-from __future__ import absolute_import
+
 
 from branchynet.links.links import *
 from branchynet.net import BranchyNet
@@ -34,17 +34,19 @@ def gen_2b(branch1, branch2):
         FL(F.max_pooling_2d, 3, 2),
         L.Linear(1024, 256),
         FL(F.relu),
-        SL(FL(F.dropout,0.5,train=True)),
+        SL(FL(F.dropout,0.5)),
         L.Linear(256, 128),
         FL(F.relu),
-        SL(FL(F.dropout,0.5,train=True)),
+        SL(FL(F.dropout,0.5)),
         Branch([L.Linear(128, 10)])
     ]
     
     return network
 
 def get_network(percentTrainKeeps=1):
-    network = gen_2b(branch1=norm() + conv(64) + conv(32) + cap(512),
-                              branch2=norm() + conv(96) + cap(128))
+    network = gen_2b(
+        branch1=norm() + conv(64) + conv(32) + cap(512),
+            branch2=norm() + conv(96) + cap(128))
+
     net = BranchyNet(network, percentTrainKeeps=percentTrainKeeps)
     return net
diff --git a/networks/lenet_mnist.py b/networks/lenet_mnist.py
index 8e34b9b..c7eff8c 100644
--- a/networks/lenet_mnist.py
+++ b/networks/lenet_mnist.py
@@ -1,4 +1,4 @@
-from __future__ import absolute_import
+
 
 from branchynet.links.links import *
 from branchynet.net import BranchyNet
@@ -6,21 +6,51 @@ from branchynet.net import BranchyNet
 import chainer.links as L
 import chainer.functions as F
 
-def get_network(percentTrainKeeps=1):
+from shrub import add_branches
+
+def get_network(percentTrainKeeps=1, branch_locs=[]):
     network = [
-        L.Convolution2D(1, 5, 5,stride=1, pad=3),
-        Branch([FL(F.max_pooling_2d, 2, 2), FL(F.ReLU()), L.Convolution2D(5, 10,  3, pad=1, stride=1),
-              FL(F.max_pooling_2d, 2, 2), FL(F.ReLU()), L.Linear(640, 10)]),
+        L.Convolution2D(1, 5, 5, stride=1, pad=3),
         FL(F.max_pooling_2d, 2, 2),
-        FL(F.ReLU()),
-        L.Convolution2D(5, 10, 5,stride=1, pad=3),
+        FL(F.relu),
+        L.Convolution2D(5, 10, 5, stride=1, pad=3),
         FL(F.max_pooling_2d, 2, 2),
-        FL(F.ReLU()),
-        L.Convolution2D(10, 20, 5,stride=1, pad=3),
+        FL(F.relu),
+        L.Convolution2D(10, 20, 5, stride=1, pad=3),
         FL(F.max_pooling_2d, 2, 2),
-        FL(F.ReLU()),
+        FL(F.relu),
         L.Linear(720, 84),
         Branch([L.Linear(84, 10)])
     ]
+    add_branches(network, branch_locs)
     net = BranchyNet(network, percentTrainKeeps=percentTrainKeeps)
     return net
+
+# # def get_network(percentTrainKeeps=1):
+#     network = [
+#         L.Convolution2D(1, 5, 5, stride=1, pad=3),
+#         Branch(
+#             [FL(F.max_pooling_2d, 2, 2), 
+#             FL(F.relu), 
+#             L.Convolution2D(5, 10, 3, pad=1, stride=1),
+#             FL(F.max_pooling_2d, 2, 2), 
+#             FL(F.relu), 
+#             L.Linear(640, 10)]),
+#         FL(F.max_pooling_2d, 2, 2),
+#         FL(F.relu),
+#         L.Convolution2D(5, 10, 5, stride=1, pad=3),
+#         FL(F.max_pooling_2d, 2, 2),
+#         FL(F.relu),
+#         L.Convolution2D(10, 20, 5, stride=1, pad=3),
+        
+#         Branch([FL(F.max_pooling_2d, 2, 2), 
+#             FL(F.relu), 
+#             L.Linear(720, 10)]),  # Added branch
+       
+#         FL(F.max_pooling_2d, 2, 2),
+#         FL(F.relu),
+#         L.Linear(720, 84),
+#         Branch([L.Linear(84, 10)])
+#     ]
+# #     net = BranchyNet(network, percentTrainKeeps=percentTrainKeeps)
+# #     return net
diff --git a/networks/resnet_cifar10.py b/networks/resnet_cifar10.py
index 302b00f..172846f 100644
--- a/networks/resnet_cifar10.py
+++ b/networks/resnet_cifar10.py
@@ -1,4 +1,4 @@
-from __future__ import absolute_import
+
 
 from branchynet.links.links import *
 from branchynet.links import resnet
@@ -35,6 +35,12 @@ def get_network(percentTrainKeeps=1):
 
     network += [Branch([resnet.ResBlock(16, 16), L.Linear(14400, 10)])]
 
+    # Adding a branch at a deeper point in the network
+    network += [Branch([resnet.ResBlock(32, 32), L.Linear(7200, 10)])]
+
+    # Final branch at a later stage, before average pooling
+    network += [Branch([resnet.ResBlock(64, 64), L.Linear(3600, 10)])]
+
     for i in range(n):
         network += [resnet.ResBlock(32 if i > 0 else 16, 32,
                                        1 if i > 0 else 2)]
diff --git a/requirements.txt b/requirements.txt
index 42372ca..98c2434 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,35 +1,30 @@
-appdirs==1.4.3
-backports.functools-lru-cache==1.6.1
-chainer==1.17.0
-cloudpickle==1.3.0
-configparser==4.0.2
-contextlib2==0.6.0.post1
-cycler==0.10.0
-decorator==4.4.2
-dill==0.3.3
-distlib==0.3.0
-filelock==3.0.12
-importlib-metadata==1.6.0
-importlib-resources==1.4.0
-kiwisolver==1.1.0
-matplotlib==2.2.5
-networkx==2.2
+appdirs==1.4.4
+chainer==7.8.1
+cloudpickle==2.2.1
+setuptools
+cycler==0.11.0
+decorator==5.1.0
+dill==0.3.6
+distlib==0.3.6
+filelock==3.13.0
+importlib-metadata==6.0.0
+importlib-resources==5.12.0
+kiwisolver==1.4.4
+matplotlib==3.6.3
+networkx==3.1
 nose==1.3.7
-numpy==1.16.6
-pathlib2==2.3.5
-Pillow==6.2.2
-protobuf==3.14.0
-pyparsing==2.4.7
-python-dateutil==2.8.1
-pytz==2020.4
-PyWavelets==1.0.3
-scandir==1.10.0
-scikit-image==0.14.5
-scikit-learn==0.20.4
-scipy==1.2.3
-singledispatch==3.4.0.3
-six==1.14.0
-subprocess32==3.5.4
-typing==3.7.4.1
-virtualenv==20.0.18
-zipp==1.2.0
+numpy==1.19.5
+Pillow==9.5.0
+protobuf==3.20.3
+pyparsing==3.1.1
+python-dateutil==2.8.2
+pytz==2023.3
+PyWavelets==1.4.1
+scikit-image==0.17.2
+scikit-learn==1.0.2
+scipy==1.5.4
+six==1.16.0
+virtualenv==20.24.1
+zipp==3.16.2
+pandas==1.2.5
+cupy-cuda102==7.8.0
\ No newline at end of file
